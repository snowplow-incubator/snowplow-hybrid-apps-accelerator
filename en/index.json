[
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/introduction/",
	"title": "Advanced Analytics for Hybrid Mobile Apps",
	"tags": [],
	"description": "",
	"content": "\nIntroduction Welcome to the Advanced Analytics for Hybrid Mobile Apps accelerator. Once finished, you will be able to build a deeper understanding of customer behaviour on your mobile apps and use your data to influence business decisions.\nHere you will learn to:\nModel and Visualise Snowplow data using the snowplow-mobile dbt package and Streamlit using our sample data for Snowflake (no need to have a working pipeline) Set-up Snowplow Tracking in a hybrid mobile app track events both from a native iOS/Android code as well as embedded Web views Apply what you have learned on your own pipeline to gain insights Hybrid apps are mobile apps that in addition to a native interface, provide part of the UI through an embedded Web view. Snowplow events are tracked from both the native code (e.g., written in Swift or Kotlin) as well as the Web view (in JavaScript). Our goal is to have both events tracked from the native code as well as the Web view share the same session and appear as tracked with the same tracker.\nSystem overview The diagram below gives a complete overview of the system covered in this accelerator:\nEvents are tracked from app logic both inside the Web view as well as the native app code. Native code events are tracked using the Snowplow iOS or Android tracker. Web view events are tracked using the WebView tracker that passes them to be tracked by the Snowplow iOS or Android tracker. Tracked events are loaded into a Snowflake warehouse by the Snowplow BDP or Open Source Cloud. The raw events are modeled into higher level entities such as screen views, sessions, or users using the snowplow-mobile dbt package. Finally, we visualize the modeled data using Streamlit. flowchart TB subgraph hybridApp[Hybrid Mobile App] subgraph webView[Web View] webViewCode[App logic] webViewTracker[Snowplow WebView tracker] webViewCode -- \"Tracks events\" --\u003e webViewTracker style webViewTracker fill:#f5f5f5,stroke:#6638B8,stroke-width:3px click webViewTracker \"https://github.com/snowplow-incubator/snowplow-webview-tracker\" \"Open tracker package\" _blank end subgraph nativeCode[Native iOS/Android] nativeAppCode[App logic] nativeTracker[Snowplow iOS/Android tracker] nativeAppCode -- \"Tracks events\" --\u003e nativeTracker style nativeTracker fill:#f5f5f5,stroke:#6638B8,stroke-width:3px click nativeTracker \"https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/installation-and-set-up/\" \"Open tracker docs\" _blank end webViewTracker -- \"Forwards events\" --\u003e nativeTracker end subgraph cloud[Cloud] snowplow[Snowplow BDP/OS Cloud] snowflake[(Snowflake)] dbt[snowplow-mobile dbt package] streamlit[Streamlit] snowplow -- \"Loads raw events\" --\u003e snowflake dbt -- \"Models data\" --\u003e snowflake streamlit -- \"Visualises modeled data\" --\u003e snowflake style dbt fill:#f5f5f5,stroke:#6638B8,stroke-width:3px click dbt \"https://docs.snowplowanalytics.com/docs/modeling-your-data/the-snowplow-mobile-data-model/dbt-mobile-data-model/\" \"Open dbt package\" _blank click snowplow \"https://snowplow.io/snowplow-bdp/\" \"Snowplow BDP\" _blank end nativeTracker -- \"Sends tracked events\" --\u003e snowplow Who is this guide for? Data practitioners who would like to get familiar with Snowplow data. Data practitioners who would like to set up tracking in a mobile hybrid app and learn how to use the Snowplow mobile data model to gain insight from their customers\u0026rsquo; behavioural data as quickly as possible. What you will learn In approximately 2 working days (~13 working hours) you can achieve the following:\nUpload data - Upload a sample Snowplow events dataset to your Snowflake warehouse Model - Configure and run the snowplow-mobile data model Visualise - Visualise the modeled data with Streamlit Track - Set-up and deploy tracking to your hybrid mobile app Next steps - Gain value from your own pipeline data through modeling and visualisation gantt dateFormat HH-mm axisFormat %M section 1. Upload 1h :upload, 00-00, 1m section 2. Model 2h :model, after upload, 2m section 3. Visualise 3h :visualise, after model, 3m section 4. Track 6h :track, after visualise, 6m section 5. Next steps 1h :next steps, after track, 2m Prerequisites Modeling and Visualisation\ndbt CLI installed / dbt Cloud account available New dbt project created and configured Python 3 Installed Snowflake account and a user with access to create schemas and tables Tracking\nSnowplow pipeline Hybrid mobile app to implement tracking on Please note that Snowflake will be used for illustration but the snowplow-mobile dbt package also supports BigQuery, Databricks, Postgres and Redshift. Further adapter support for this accelerator coming soon!\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/upload/1-upload/",
	"title": "Data upload",
	"tags": [],
	"description": "",
	"content": "There are a number of options to load the sample Snowplow data to your Snowflake warehouse. Select the most suitable for your project below.\nAttachments upload.zip (118 KB) Python Snowflake Web Interface Download the upload.zip folder which contains the sample_events.csv and the snowflake_upload.py files. You will need both to load the sample data to the Snowflake warehouse with Python.\nStep 1: Set up your environment Set up a virtual environment (recommended) and install the snowflake-connector-python package (tested with version 2.7.12).\npython3 -m venv env source env/bin/activate pip install snowflake-connector-python==2.7.12 Step 2: Change variables and connection details Open the snowflake_upload.py file and edit the following variables.\n2.1 Connection details - update username, password and account conn=sf.connect(user=\u0026#39;your_username\u0026#39;,password=\u0026#39;your_password\u0026#39;,account=\u0026#39;your_account\u0026#39;) 2.2 Variables to be modified - warehouse and database warehouse=\u0026#39;YOUR_WAREHOUSE\u0026#39; database = \u0026#39;YOUR_DB 2.3 Path to the sample_data.csv csv_file = \u0026#39;/Users/your_user/path_to_csv/sample_events.csv\u0026#39; Step 3: Upload Data Run snowflake_upload.py.\npython3 snowflake_upload.py It should finish execution within a minute. You should be alerted as soon as each intermediary step finishes:\nSchema created Staging table YOUR_DB.ATOMIC.SAMPLE_EVENTS_STAGED is created Stage dropped, if applicable Stage created File put to stage Data loaded into staging table Target table: YOUR_DB.ATOMIC.SAMPLE_EVENTS is created Staging table: YOUR_DB.ATOMIC.SAMPLE_EVENTS_STAGED is dropped You will now have the ATOMIC.SAMPLE_EVENTS created and loaded with sample data.\nDownload the upload.zip folder which contains the sample_events.csv and the snowflake_upload.py files. You will only need sample_events.csv to load the sample data to the Snowflake warehouse using the Snowflake Web Interface.\nFor more details please check out the official Snowflake documentation.\nStep 1: Create the ATOMIC schema If the ATOMIC schema doesn\u0026rsquo;t exist, create it in your target database.\nCREATE SCHEMA IF NOT EXISTS TARGET_DB.ATOMIC Step 2: Create the SAMPLE_EVENTS_BASE table This is where you will load the sample data to. You will need to modify the TARGET_DB according to your own database.\nCREATE OR REPLACE TABLE TARGET_DB.ATOMIC.SAMPLE_EVENTS_BASE ( APP_ID VARCHAR(255), PLATFORM VARCHAR(255), ETL_TSTAMP TIMESTAMP_NTZ(9), COLLECTOR_TSTAMP TIMESTAMP_NTZ(9) NOT NULL, DVCE_CREATED_TSTAMP TIMESTAMP_NTZ(9), EVENT VARCHAR(128), EVENT_ID VARCHAR(36) NOT NULL, TXN_ID NUMBER(38,0), NAME_TRACKER VARCHAR(128), V_TRACKER VARCHAR(100), V_COLLECTOR VARCHAR(100) NOT NULL, V_ETL VARCHAR(100) NOT NULL, USER_ID VARCHAR(255), USER_IPADDRESS VARCHAR(128), USER_FINGERPRINT VARCHAR(128), DOMAIN_USERID VARCHAR(128), DOMAIN_SESSIONIDX NUMBER(38,0), NETWORK_USERID VARCHAR(128), GEO_COUNTRY VARCHAR(2), GEO_REGION VARCHAR(3), GEO_CITY VARCHAR(75), GEO_ZIPCODE VARCHAR(15), GEO_LATITUDE FLOAT, GEO_LONGITUDE FLOAT, GEO_REGION_NAME VARCHAR(100), IP_ISP VARCHAR(100), IP_ORGANIZATION VARCHAR(128), IP_DOMAIN VARCHAR(128), IP_NETSPEED VARCHAR(100), PAGE_URL VARCHAR(4096), PAGE_TITLE VARCHAR(2000), PAGE_REFERRER VARCHAR(4096), PAGE_URLSCHEME VARCHAR(16), PAGE_URLHOST VARCHAR(255), PAGE_URLPORT NUMBER(38,0), PAGE_URLPATH VARCHAR(3000), PAGE_URLQUERY VARCHAR(6000), PAGE_URLFRAGMENT VARCHAR(3000), REFR_URLSCHEME VARCHAR(16), REFR_URLHOST VARCHAR(255), REFR_URLPORT NUMBER(38,0), REFR_URLPATH VARCHAR(6000), REFR_URLQUERY VARCHAR(6000), REFR_URLFRAGMENT VARCHAR(3000), REFR_MEDIUM VARCHAR(25), REFR_SOURCE VARCHAR(50), REFR_TERM VARCHAR(255), MKT_MEDIUM VARCHAR(255), MKT_SOURCE VARCHAR(255), MKT_TERM VARCHAR(255), MKT_CONTENT VARCHAR(500), MKT_CAMPAIGN VARCHAR(255), SE_CATEGORY VARCHAR(1000), SE_ACTION VARCHAR(1000), SE_LABEL VARCHAR(4096), SE_PROPERTY VARCHAR(1000), SE_VALUE FLOAT, TR_ORDERID VARCHAR(255), TR_AFFILIATION VARCHAR(255), TR_TOTAL NUMBER(18,2), TR_TAX NUMBER(18,2), TR_SHIPPING NUMBER(18,2), TR_CITY VARCHAR(255), TR_STATE VARCHAR(255), TR_COUNTRY VARCHAR(255), TI_ORDERID VARCHAR(255), TI_SKU VARCHAR(255), TI_NAME VARCHAR(255), TI_CATEGORY VARCHAR(255), TI_PRICE NUMBER(18,2), TI_QUANTITY NUMBER(38,0), PP_XOFFSET_MIN NUMBER(38,0), PP_XOFFSET_MAX NUMBER(38,0), PP_YOFFSET_MIN NUMBER(38,0), PP_YOFFSET_MAX NUMBER(38,0), USERAGENT VARCHAR(1000), BR_NAME VARCHAR(50), BR_FAMILY VARCHAR(50), BR_VERSION VARCHAR(50), BR_TYPE VARCHAR(50), BR_RENDERENGINE VARCHAR(50), BR_LANG VARCHAR(255), BR_FEATURES_PDF BOOLEAN, BR_FEATURES_FLASH BOOLEAN, BR_FEATURES_JAVA BOOLEAN, BR_FEATURES_DIRECTOR BOOLEAN, BR_FEATURES_QUICKTIME BOOLEAN, BR_FEATURES_REALPLAYER BOOLEAN, BR_FEATURES_WINDOWSMEDIA BOOLEAN, BR_FEATURES_GEARS BOOLEAN, BR_FEATURES_SILVERLIGHT BOOLEAN, BR_COOKIES BOOLEAN, BR_COLORDEPTH VARCHAR(12), BR_VIEWWIDTH NUMBER(38,0), BR_VIEWHEIGHT NUMBER(38,0), OS_NAME VARCHAR(50), OS_FAMILY VARCHAR(50), OS_MANUFACTURER VARCHAR(50), OS_TIMEZONE VARCHAR(255), DVCE_TYPE VARCHAR(50), DVCE_ISMOBILE BOOLEAN, DVCE_SCREENWIDTH NUMBER(38,0), DVCE_SCREENHEIGHT NUMBER(38,0), DOC_CHARSET VARCHAR(128), DOC_WIDTH NUMBER(38,0), DOC_HEIGHT NUMBER(38,0), TR_CURRENCY VARCHAR(3), TR_TOTAL_BASE NUMBER(18,2), TR_TAX_BASE NUMBER(18,2), TR_SHIPPING_BASE NUMBER(18,2), TI_CURRENCY VARCHAR(3), TI_PRICE_BASE NUMBER(18,2), BASE_CURRENCY VARCHAR(3), GEO_TIMEZONE VARCHAR(64), MKT_CLICKID VARCHAR(128), MKT_NETWORK VARCHAR(64), ETL_TAGS VARCHAR(500), DVCE_SENT_TSTAMP TIMESTAMP_NTZ(9), REFR_DOMAIN_USERID VARCHAR(128), REFR_DVCE_TSTAMP TIMESTAMP_NTZ(9), DOMAIN_SESSIONID VARCHAR(128), DERIVED_TSTAMP TIMESTAMP_NTZ(9), EVENT_VENDOR VARCHAR(1000), EVENT_NAME VARCHAR(1000), EVENT_FORMAT VARCHAR(128), EVENT_VERSION VARCHAR(128), EVENT_FINGERPRINT VARCHAR(128), TRUE_TSTAMP TIMESTAMP_NTZ(9), LOAD_TSTAMP TIMESTAMP_NTZ(9), CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_UA_PARSER_CONTEXT_1 VARCHAR, CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_WEB_PAGE_1 VARCHAR, CONTEXTS_COM_IAB_SNOWPLOW_SPIDERS_AND_ROBOTS_1 VARCHAR, CONTEXTS_NL_BASJES_YAUAA_CONTEXT_1 VARCHAR, constraint EVENT_ID_PK primary key (EVENT_ID) ); Step 3: Load the data 3.1 Log into your web interface and click on Databases tab.\n3.2 Locate the SAMPLE_EVENTS_BASE table that you just created and select it.\n3.3 Click the Load Data button to open the Load Data wizard.\n3.4 Select the relevant warehouse from the dropdown list. Click Next.\n3.5 Within the Source Files section select Load files from your computer option and click the Select Files button. If you have not saved the sample file provided as an attachment above please do so. Navigate to the SAMPLE_EVENTS.csv and click the Upload then the Next button.\n3.6 Create a new File Format with the plus (+) symbol beside the dropdown list, give it a name and change the following settings of the default csv file formats:\nHeader lines to skip= 1 Field optionally enclosed by= Double Quote 3.7 Click the Load button (no need to alter the Load Options). Loading should take place within a couple of minutes.\nFor more details please check out the official Snowflake documentation.\nStep 4: Create the ATOMIC.SAMPLE_EVENTS table The Snowplow pipeline creates context fields as arrays but uploading the test data can be achieved through string/varchar data type first. Run the below DDL statement in your SQL editor to create the sample_events table from the base table including the necessary conversions:\nCREATE OR REPLACE TABLE TARGET_DB.ATOMIC.SAMPLE_EVENTS AS ( SELECT APP_ID, PLATFORM, ETL_TSTAMP, COLLECTOR_TSTAMP, DVCE_CREATED_TSTAMP, EVENT, EVENT_ID, TXN_ID, NAME_TRACKER, V_TRACKER, V_COLLECTOR, V_ETL, USER_ID, USER_IPADDRESS, USER_FINGERPRINT, DOMAIN_USERID, DOMAIN_SESSIONIDX, NETWORK_USERID, GEO_COUNTRY, GEO_REGION, GEO_CITY, GEO_ZIPCODE, GEO_LATITUDE, GEO_LONGITUDE, GEO_REGION_NAME, IP_ISP, IP_ORGANIZATION, IP_DOMAIN, IP_NETSPEED, PAGE_URL, PAGE_TITLE, PAGE_REFERRER, PAGE_URLSCHEME, PAGE_URLHOST, PAGE_URLPORT, PAGE_URLPATH, PAGE_URLQUERY, PAGE_URLFRAGMENT, REFR_URLSCHEME, REFR_URLHOST, REFR_URLPORT, REFR_URLPATH, REFR_URLQUERY, REFR_URLFRAGMENT, REFR_MEDIUM, REFR_SOURCE, REFR_TERM, MKT_MEDIUM, MKT_SOURCE, MKT_TERM, MKT_CONTENT, MKT_CAMPAIGN, SE_CATEGORY, SE_ACTION, SE_LABEL, SE_PROPERTY, SE_VALUE, TR_ORDERID, TR_AFFILIATION, TR_TOTAL, TR_TAX, TR_SHIPPING, TR_CITY, TR_STATE, TR_COUNTRY, TI_ORDERID, TI_SKU, TI_NAME, TI_CATEGORY, TI_PRICE, TI_QUANTITY, PP_XOFFSET_MIN, PP_XOFFSET_MAX, PP_YOFFSET_MIN, PP_YOFFSET_MAX, REPLACE(USERAGENT, \u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;) as USERAGENT, BR_NAME, BR_FAMILY, BR_VERSION, BR_TYPE, BR_RENDERENGINE, BR_LANG, BR_FEATURES_PDF, BR_FEATURES_FLASH, BR_FEATURES_JAVA, BR_FEATURES_DIRECTOR, BR_FEATURES_QUICKTIME, BR_FEATURES_REALPLAYER, BR_FEATURES_WINDOWSMEDIA, BR_FEATURES_GEARS, BR_FEATURES_SILVERLIGHT, BR_COOKIES, BR_COLORDEPTH, BR_VIEWWIDTH, BR_VIEWHEIGHT, OS_NAME, OS_FAMILY, OS_MANUFACTURER, OS_TIMEZONE, DVCE_TYPE, DVCE_ISMOBILE, DVCE_SCREENWIDTH, DVCE_SCREENHEIGHT, DOC_CHARSET, DOC_WIDTH, DOC_HEIGHT, TR_CURRENCY, TR_TOTAL_BASE, TR_TAX_BASE, TR_SHIPPING_BASE, TI_CURRENCY, TI_PRICE_BASE, BASE_CURRENCY, GEO_TIMEZONE, MKT_CLICKID, MKT_NETWORK, ETL_TAGS, DVCE_SENT_TSTAMP, REFR_DOMAIN_USERID, REFR_DVCE_TSTAMP, DOMAIN_SESSIONID, DERIVED_TSTAMP, EVENT_VENDOR, EVENT_NAME, EVENT_FORMAT, EVENT_VERSION, EVENT_FINGERPRINT, TRUE_TSTAMP, LOAD_TSTAMP, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_UA_PARSER_CONTEXT_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_UA_PARSER_CONTEXT_1, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_WEB_PAGE_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_WEB_PAGE_1, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_COM_IAB_SNOWPLOW_SPIDERS_AND_ROBOTS_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_COM_IAB_SNOWPLOW_SPIDERS_AND_ROBOTS_1, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_NL_BASJES_YAUAA_CONTEXT_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_NL_BASJES_YAUAA_CONTEXT_1 FROM ATOMIC.SAMPLE_EVENTS_BASE ) Step 5: Drop the SAMPLE_EVENTS_BASE table DROP TABLE TARGET_DB.ATOMIC.SAMPLE_EVENTS_BASE You will now have the ATOMIC.SAMPLE_EVENTS created and loaded with sample data.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/modeling/1-installation/",
	"title": "Install Snowplow dbt Package",
	"tags": [],
	"description": "",
	"content": "Step 1: Add snowplow-mobile package Add the snowplow-web package to your packages.yml file. The latest version can be found here\npackages: - package: snowplow/snowplow_mobile version: 0.5.3 Step 2: Install the package Install the package by running:\ndbt deps "
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/tracking/1-installation/",
	"title": "Installing the trackers",
	"tags": [],
	"description": "",
	"content": "To instrument tracking, you will need to install tracker libraries both in the Web view as well as the native mobile app.\nSnowplow WebView tracker installation To install the WebView tracker in your JavaScript or TypeScript app, add the npm package:\nnpm install --save @snowplow/webview-tracker You will then be able to use the functions provided by the WebView tracker as follows:\nimport { trackSelfDescribingEvent } from \u0026#39;@snowplow/webview-tracker\u0026#39;; There is no need to configure the WebView tracker. All configuration is done in the native layer as explained next.\nSnowplow iOS and Android tracker installation First, you will need to install the Snowplow tracker package in your app. Below, we show how to do so using the Swift Package Manager (SPM) on iOS and Gradle on Android. To learn about other options for installing the trackers (e.g., using CocoaPods or Carthage on iOS), see the mobile tracker documentation.\niOS Android You can install the tracker using SPM as follows:\nIn Xcode, select File \u0026gt; Swift Packages \u0026gt; Add Package Dependency. Add the url where to download the library: https://github.com/snowplow/snowplow-objc-tracker The tracker can be installed using Gradle. Add the following to your build.gradle file:\ndependencies { ... // Snowplow Android Tracker implementation \u0026#39;com.snowplowanalytics:snowplow-android-tracker:3.+\u0026#39; // In case \u0026#39;lifecycleAutotracking\u0026#39; is enabled implementation \u0026#39;androidx.lifecycle-extensions:2.2.+\u0026#39; ... } "
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/next_steps/1-run_model/",
	"title": "Model your pipeline data",
	"tags": [],
	"description": "",
	"content": "At this stage you should:\nHave tracking set-up Have some data in the ATOMIC.EVENTS table Have a working dbt project with the mobile model configurations for the sample data Step 1: Complete refresh of your Snowplow mobile package (Optional) If you would like to use your current dbt environment that you set-up during modelling the sample data you might want to start from scratch.\nWhile you can drop and recompute the incremental tables within this package using the standard --full-refresh flag, all manifest tables are protected from being dropped in production. Without dropping the manifest during a full refresh, the selected derived incremental tables would be dropped but the processing of events would resume from where the package left off (as captured by the snowplow_mobile_incremental_manifest table) rather than your snowplow__start_date.\nIn order to drop all the manifest tables and start again set the snowplow__allow_refresh variable to true at run time:\ndbt run --select snowplow_mobile tag:snowplow_mobile_incremental --full-refresh --vars \u0026#39;snowplow__allow_refresh: true\u0026#39; # or using selector flag dbt run --selector snowplow_mobile --full-refresh --vars \u0026#39;snowplow__allow_refresh: true\u0026#39; Step 2: Modify variables Assuming that you followed the guide on how to run the data model on the sample data, here we will only highlight the differences in the set-up:\nRemove the snowplow__events variable. This time the base table will be the default atomic.events, therefore no need to overwrite it.\nChange the snowplow__start_date variable according to the data you have in your events table.\nOptional:\nsnowplow__backfill_limit_days: The maximum number of days of new data to be processed since the latest event processed. Set it to 1. We suggest changing snowplow__backfill_limit_days to 1 whilst working in your dev environment initially so that you can test how your incremental runs work. You will only have a few days of data available at this stage and if you leave it at the default 30 days, you will model all your data in one go.\nStep 3: Run the model Execute the following either through your CLI or from within dbt Cloud\ndbt run --selector snowplow_mobile Depending on the period of data available since the snowplow__start_date and the snowplow__backfill_limit_days variable you might not process all your data during your first run. Each time the model runs it should display the period it processes and the timestamp of the last event processed for each model within the package. This gets saved in the snowplow__incremental_manifest table so you can always check the data processing state (see below).\nStep 4: Run dbt test Run our recommended selector specified tests to identify potential issues with the data:\ndbt test --selector snowplow_mobile_lean_tests "
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/visualisation/1-streamlit/",
	"title": "Streamlit",
	"tags": [],
	"description": "",
	"content": "Streamlit uses Python to build shareable dashboards without the need for front-end development experience.\nDownload the streamlit-visualisation project template and copy the unzipped folder to your project directory to get started.\nAttachments streamlit-project-mobile.zip (39 KB) Step 1: Install requirements Run the command below to install the project requirements and run the virtual environment\n❗❗ This implementation has been tested with the following dependencies: python=3.9.13, streamlit=1.12.0, snowflake-connector-python==2.7.9. If you run into package compatibility issues or encounter any errors try using them to build your own environment.\npipenv install pipenv shell Step 2: Set-up Database Connection Open secrets.toml and add your Snowflake account and database details. Ensure secrets.toml is in .gitignore to keep your information safe.\n# .streamlit/secrets.toml [snowflake] user = \u0026#34;xxx\u0026#34; password = \u0026#34;xxx\u0026#34; account = \u0026#34;xxx\u0026#34; database = \u0026#34;xxx\u0026#34; schema = \u0026#34;xxx\u0026#34; warehouse = \u0026#34;xxx\u0026#34; Step 3: Run the Streamlit dashboard Run the command below to run the streamlit locally\nstreamlit run Dashboard.py In case the dashboard does not load due to errors such as \u0026lsquo;This session does not have a current database. Call \u0026lsquo;USE DATABASE\u0026rsquo;, or use a qualified name.\u0026rsquo; a possible workaround is to assign default ROLE to the Snowflake user that could handle this.\u0026rsquo;\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/upload/",
	"title": "Upload sample data",
	"tags": [],
	"description": "",
	"content": "Upload sample data flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Next steps) style id1 fill:#f5f5f5,stroke:#6638B8,stroke-width:3px style id2 fill:#f5f5f5,stroke:#333,stroke-width:1px style id4 fill:#f5f5f5,stroke:#333,stroke-width:1px style id5 fill:#f5f5f5,stroke:#333,stroke-width:1px style id3 fill:#f5f5f5,stroke:#333,stroke-width:1px A sample events dataset for your Snowflake warehouse has been provided. This will allow you to be able to start data modeling and getting familiar with Snowplow event data, without the need to have a working pipeline. This chapter will guide you through this process.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/tracking/2-mobile_trackers_usage/",
	"title": "Configuring and using the iOS and Android trackers",
	"tags": [],
	"description": "",
	"content": "Having installed the tracker dependencies, the next step is to initialize the tracker instances in your app. Tracker instances are initialized given configuration that includes network settings, tracker feature settings, session settings, and more.\nThe following snippets show how to initialize tracker instances using the default settings. They call the Snowplow.createTracker() function and pass it two required information:\nThe tracker namespace which uniquely identifies the tracker within the app. Network configuration with the endpoint address of the Snowplow Collector (e.g., Snowplow Micro or Snowplow Mini) to send events to. iOS Android import SnowplowTracker let networkConfig = NetworkConfiguration(endpoint: COLLECTOR_URL, method: .post) let tracker = Snowplow.createTracker( namespace: \u0026#34;appTracker\u0026#34;, network: networkConfig, configurations: [] ); import com.snowplowanalytics.snowplow.Snowplow; import com.snowplowanalytics.snowplow.network.HttpMethod; import com.snowplowanalytics.snowplow.configuration.NetworkConfiguration; NetworkConfiguration networkConfig = new NetworkConfiguration(COLLECTOR_URL, HttpMethod.POST); TrackerController tracker = Snowplow.createTracker(context, \u0026#34;appTracker\u0026#34;, networkConfig ); You can learn more about installing and configuring the mobile trackers in the mobile tracker documentation.\nTracking events in native code The initialized tracker instances can be used to track events in your native code. We won\u0026rsquo;t go into detail on all the tracking features, but only give an example how to track self-describing events. Self-describing events are based around \u0026ldquo;self-describing\u0026rdquo; (self-referential) JSONs, which are a specific kind of JSON schema. A unique schema can be designed for each type of event that you want to track. This allows you to track the specific things that are important to you, in a way that is defined by you.\nA self-describing JSON has two keys, schema and data. The schema value should point to a valid self-describing JSON schema. They are called self-describing because the schema will specify the fields allowed in the data value. Read more about how schemas are used with Snowplow here.\niOS Android let schema = \u0026#34;iglu:com.snowplowanalytics.snowplow/link_click/jsonschema/1-0-1\u0026#34; let data = [\u0026#34;targetUrl\u0026#34;: \u0026#34;http://a-target-url.com\u0026#34;] let event = SelfDescribing(schema: schema, payload: data) tracker.track(event) String schema = \u0026#34;iglu:com.snowplowanalytics.snowplow/link_click/jsonschema/1-0-1\u0026#34;; Map data = new HashMap(); data.put(\u0026#34;targetUrl\u0026#34;, \u0026#34;http://a-target-url.com\u0026#34;); SelfDescribingJson sdj = new SelfDescribingJson(schema, data); SelfDescribing event = new SelfDescribing(sdj); tracker.track(event); Subscribing to events from the Web view In addition to tracking events from the native code, we also want to track events from the Web view. In the following section, we will explain how to instrument your Web application to use the WebView tracker. However, in order for the events from the WebView tracker to arrive at the Snowplow Collector, it is necessary to subscribe the native mobile trackers to listen for messages from the Web view.\nYou can call the Snowplow.subscribeToWebViewEvents(webView) function to subscribe to the messages (same on iOS and Android). The webView object is an instance of WKWebView on iOS and WebView on Android. Please note that the events will only be tracked if you have initialized a tracker instance as described above.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/modeling/",
	"title": "Modeling",
	"tags": [],
	"description": "",
	"content": "Modeling your Data flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Next steps) style id2 fill:#f5f5f5,stroke:#6638B8,stroke-width:3px style id1 fill:#f5f5f5,stroke:#333,stroke-width:1px style id4 fill:#f5f5f5,stroke:#333,stroke-width:1px style id5 fill:#f5f5f5,stroke:#333,stroke-width:1px style id3 fill:#f5f5f5,stroke:#333,stroke-width:1px The snowplow-mobile dbt package transforms and aggregates the raw mobile event data collected from the Snowplow mobile trackers (e.g., the Android tracker, iOS tracker, React Native tracker) into a set of derived tables: screen views, sessions, users and user mappings. Modeling the data makes it easier to digest and derive business value from the Snowplow data either through AI or BI.\nIn this chapter you will learn how to set-up an run the snowplow-mobile package to model the sample data.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/modeling/2-run/",
	"title": "Set-up and run dbt Package",
	"tags": [],
	"description": "",
	"content": "This step assumes you have data in the ATOMIC.SAMPLE_EVENTS table which will be used to demonstrate how to set-up and run the snowplow-mobile dbt package to model Snowplow mobile data.\nStep 1: Set-up Variables The snowplow_mobile dbt package comes with a list of variables specified with a default value that you may need to overwrite in your own dbt project\u0026rsquo;s dbt_project.yml file. For details you can have a look at the installed package\u0026rsquo;s default variables which can be found at [dbt_project_name]/dbt_packages/snowplow_mobile/dbt_project.yml.\nFor the sake of simplicity we have selected the variables that you will most likely need to overwrite, the rest can be changed at a later stage if and when it is needed.\nsnowplow__start_date: The date of the first tracked event. snowplow__enable_mobile_context: Disabled by default, enables the mobile (or platform) context with device information snowplow__enable_geolocation_context: Disabled by default, enables geolocation context snowplow__enable_application_context: Disabled by default, enables application context snowplow__enable_screen_context: Disabled by default, enables screen context with the current screen view information snowplow__events: Variable to overwrite the events table in case it is named differently. It would have to be modified when using the sample_events table as a base. Add the following snippet to the dbt_project.yml:\nvars: snowplow_mobile: snowplow__start_date: \u0026#39;2021-09-01\u0026#39; snowplow__backfill_limit_days: 400 # in order to load the whole sample dataset snowplow__enable_mobile_context: true snowplow__enable_geolocation_context: true snowplow__enable_application_context: true snowplow__enable_screen_context: true snowplow__events: \u0026#39;atomic.sample_events\u0026#39; Step 2: Add the selectors.yml to your project The mobile package provides a suite of suggested selectors to help run and test the models.\nThese are defined in the selectors.yml file within the package, however to use these model selections you will need to copy this file into your own dbt project directory.\nThis is a top-level file and therefore should sit alongside your dbt_project.yml file.\nStep 3: Run the model Execute the following either through your CLI or from within dbt Cloud\ndbt run --selector snowplow_mobile This should take a couple of minutes to run.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/next_steps/2-custom_models/",
	"title": "Custom models",
	"tags": [],
	"description": "",
	"content": "If you have got to this stage, congratulations! You are ready to take action and use your Snowplow generated data to help your business grow.\nAs a next step you might want to check out our detailed guide on how to create custom models to adjust the snowplow-mobile data model to your own needs if the out-of-the box solution does not fully fit your needs.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/modeling/3-test/",
	"title": "Explore Snowplow data",
	"tags": [],
	"description": "",
	"content": "Data should now be loaded into your warehouse. In this section, we will take a closer look at the output to mitigate data issues and get familiar with the derived tables.\nStep 1: Check the output schemas Head to the SQL editor of your choice (e.g.: Snowflake Web UI) to check the model\u0026rsquo;s output. You should be able to see three new schemas created:\n[your_custom_schema]_scratch: drop and recompute models that aid the incremental run [your_custom_schema]_derived: main output models you can use in your downstream models and reporting [your_custom_schema]_manifest: tables that help the integrity and core incremental logic of the model Step 2: Explore your data Take some time to familiarise yourself with the derived tables. You could run a few simple queries such as the ones listed below. Make sure to modify the schema to be aligned with your custom dbt schema.\nFind out the number of screen views using derived.snowplow_mobile_screen_views:\nWITH VIEWS AS ( SELECT SCREEN_VIEW_NAME, COUNT(*) FROM YOUR_CUSTOM_SCHEMA_DERIVED.SNOWPLOW_MOBILE_SCREEN_VIEWS GROUP BY 1 ORDER BY 2 DESC ) SELECT * FROM VIEWS Calculate the bounce rate using derived.snowplow_sessions:\nWITH BOUNCE_RATE AS ( SELECT APP_ID, COUNT(DISTINCT SESSION_ID) AS SESSIONS, COUNT(DISTINCT CASE WHEN SCREEN_VIEWS = 1 THEN SESSION_ID END) / COUNT(DISTINCT SESSION_ID) AS BOUNCE_RATE FROM YOUR_CUSTOM_SCHEMA_DERIVED.SNOWPLOW_MOBILE_SESSIONS GROUP BY 1 ORDER BY SESSIONS DESC ) SELECT * FROM BOUNCE_RATE Find out details about the highest engaged user using derived.snowplow_users:\nWITH ENGAGEMENT AS ( SELECT * FROM YOUR_CUSTOM_SCHEMA_DERIVED.SNOWPLOW_MOBILE_USERS ORDER BY SCREEN_VIEWS DESC LIMIT 1 ) SELECT * FROM ENGAGEMENT Check out the database section of the documentation site for a full breakdown of what the output should look like.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/tracking/3-webview_usage/",
	"title": "Setting up tracking in Web views",
	"tags": [],
	"description": "",
	"content": "In the Installation section, you installed the WebView tracker in your JavaScript Web app that is accessed in the mobile Web views. After that, you learned how to configure and use the mobile trackers to track events and subscribe to events from Web views. This section explains how to use the WebView tracker to track events inside Web views.\nEvent tracking API The tracker provides a set of functions to manually track events. The functions range from single purpose ones, such as trackScreenView, to the more complex but flexible trackSelfDescribingEvent, which can be used to track any kind of user behaviour. We strongly recommend using trackSelfDescribingEvent for your tracking, as it allows you to design custom event types to match your business requirements. This post on our blog, \u0026ldquo;Re-thinking the structure of event data\u0026rdquo; might be informative here.\nYou can import the functions from the @snowplow/webview-tracker package:\nimport { trackSelfDescribingEvent, trackScreenView } from \u0026#39;@snowplow/webview-tracker\u0026#39;; The following functions are available:\nMethod Event type tracked trackSelfDescribingEvent Track a custom event based on \u0026ldquo;self-describing\u0026rdquo; JSON schema trackStructEvent Track a semi-custom structured event trackScreenView Track a view of a screen in the app (to be used with the Snowplow mobile data model) trackPageView Track a Web page visit (to be used with the Snowplow web data model) All the methods share common features and parameters. Every type of event can have an optional context added. See the end of this section to learn about adding context entities to events. It\u0026rsquo;s important to understand how event context works, as it is one of the most powerful Snowplow features. Adding event context entities is a way to add depth, richness and value to all of your events.\nAll of the trackXYZ() methods accept two arguments:\nArgument Description Required? event Event body, depends on the event being tracked Yes trackers Optional list of tracker namespaces to track the event with (undefined for default tracker) No For instance, the following tracks a structured event (explained below) using a tracker initialized with the namespace ns1:\ntrackStructEvent( { category: \u0026#39;shop\u0026#39;, action: \u0026#39;add-to-basket\u0026#39; }, [\u0026#39;ns1\u0026#39;] ); Track self-describing events with trackSelfDescribingEvent Use the trackSelfDescribingEvent function to track a custom event. This is the most advanced and powerful tracking method, which requires a certain amount of planning and infrastructure.\nSelf-describing events are based around \u0026ldquo;self-describing\u0026rdquo; (self-referential) JSONs, which are a specific kind of JSON schema. A unique schema can be designed for each type of event that you want to track. This allows you to track the specific things that are important to you, in a way that is defined by you.\nThis is particularly useful when:\nYou want to track event types which are proprietary/specific to your business You want to track events which have unpredictable or frequently changing properties A self-describing JSON has two keys, schema and data. The schema value should point to a valid self-describing JSON schema. They are called self-describing because the schema will specify the fields allowed in the data value. Read more about how schemas are used with Snowplow here.\nArgument Description Required? event.schema The grouping of structured events which this action belongs to Yes event.data Defines the type of user interaction which this event involves Yes context List of context entities as self-describing JSONs No Example using the default tracker:\ntrackSelfDescribingEvent({ event: { schema: \u0026#39;iglu:com.example_company/save_game/jsonschema/1-0-2\u0026#39;, data: { \u0026#39;saveId\u0026#39;: \u0026#39;4321\u0026#39;, \u0026#39;level\u0026#39;: 23, \u0026#39;difficultyLevel\u0026#39;: \u0026#39;HARD\u0026#39;, \u0026#39;dlContent\u0026#39;: true } } }); Track structured events with Structured This method provides a halfway-house between tracking fully user-defined self-describing events and out-of-the box predefined events. This event type can be used to track many types of user activity, as it is somewhat customizable. \u0026ldquo;Struct\u0026rdquo; events closely mirror the structure of Google Analytics events, with \u0026ldquo;category\u0026rdquo;, \u0026ldquo;action\u0026rdquo;, \u0026ldquo;label\u0026rdquo;, and \u0026ldquo;value\u0026rdquo; properties.\nAs these fields are fairly arbitrary, we recommend following the advice in this table how to define structured events. It\u0026rsquo;s important to be consistent throughout the business about how each field is used.\nArgument Description Required? category The grouping of structured events which this action belongs to Yes action Defines the type of user interaction which this event involves Yes label Often used to refer to the \u0026lsquo;object\u0026rsquo; the action is performed on No property Describing the \u0026lsquo;object\u0026rsquo;, or the action performed on it No value Provides numerical data about the event No context List of context entities as self-describing JSONs No Example:\ntrackStructEvent({ category: \u0026#39;shop\u0026#39;, action: \u0026#39;add-to-basket\u0026#39;, label: \u0026#39;Add To Basket\u0026#39;, property: \u0026#39;pcs\u0026#39;, value: 2.00, }); Track screen views with trackScreenView Use ScreenView to track a user viewing a screen (or similar) within your app. This is the page view equivalent for apps that are not webpages. Screen view events are used in the Snowplow mobile data model.\nThis method creates an unstruct event, by creating and tracking a self-describing event. The schema ID for this is \u0026ldquo;iglu:com.snowplowanalytics.snowplow/screen_view/jsonschema/1-0-0\u0026rdquo;, and the data field will contain the parameters which you provide. That schema is hosted on the schema repository Iglu Central, and so will always be available to your pipeline.\nArgument Description Required? name The human-readable name of the screen viewed. Yes id The id (UUID v4) of screen that was viewed. Yes type The type of screen that was viewed. No previousName The name of the previous screen that was viewed. No previousType The type of screen that was viewed. No previousId The id (UUID v4) of the previous screen that was viewed. No transitionType The type of transition that led to the screen being viewed. No context List of context entities as self-describing JSONs No Example:\ntrackScreenView({ id: \u0026#39;2c295365-eae9-4243-a3ee-5c4b7baccc8f\u0026#39;, name: \u0026#39;home\u0026#39;, type: \u0026#39;full\u0026#39;, transitionType: \u0026#39;none\u0026#39; }); Track Web page views with trackPageView The PageViewEvent may be used to track page views on the Web. The event is designed to track web page views and automatically captures page title, referrer and URL.\nPage view events are the basic building blocks for the Snowplow web data model. For mobile apps we recommend using the mobile data model and tracking screen view events instead.\nArgument Description Required? title Override the page title. No context List of context entities as self-describing JSONs No trackPageView(); Adding data to your events using context entities Event context is an incredibly powerful aspect of Snowplow tracking, which allows you to create very rich data. It is based on the same self-describing JSON schemas as the self-describing events. Using event context, you can add any details you like to your events, as long as you can describe them in a self-describing JSON schema.\nEach schema will describe a single \u0026ldquo;entity\u0026rdquo;. All of an event\u0026rsquo;s entities together form the event context. The event context will be sent as one field of the event, finally ending up in one column (context) in your data storage. There is no limit to how many entities can be attached to one event.\nNote that context can be added to any event type, not just self-describing events. This means that even a simple event type like a page view can hold complex and extensive information – reducing the chances of data loss and the amount of modelling (JOINs etc.) needed in modelling, while increasing the value of each event, and the sophistication of the possible use cases.\nThe entities you provide are validated against their schemas as the event is processed (during the enrich phase). If there is a mistake or mismatch, the event is processed as a Bad Event.\nOnce defined, an entity can be attached to any kind of event. This is also an important point; it means your tracking is as DRY as possible. Using the same \u0026ldquo;user\u0026rdquo; or \u0026ldquo;image\u0026rdquo; or \u0026ldquo;search result\u0026rdquo; (etc.) entities throughout your tracking reduces error, and again makes the data easier to model.\nExample:\ntrackStructEvent( { category: \u0026#39;shop\u0026#39;, action: \u0026#39;add-to-basket\u0026#39;, label: \u0026#39;Add To Basket\u0026#39;, property: \u0026#39;pcs\u0026#39;, value: 2.00, context: [ { schema: \u0026#39;iglu:com.my_company/movie_poster/jsonschema/1-0-0\u0026#39;, data: { movie_name: \u0026#39;Solaris\u0026#39;, poster_country: \u0026#39;JP\u0026#39;, poster_date: \u0026#39;1978-01-01\u0026#39; } }, { schema: \u0026#39;iglu:com.my_company/customer/jsonschema/1-0-0\u0026#39;, data: { p_buy: 0.23, segment: \u0026#39;young_adult\u0026#39; } } ] }, [\u0026#39;ns1\u0026#39;] ); "
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/visualisation/",
	"title": "Visualisation",
	"tags": [],
	"description": "",
	"content": " Visualisation flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Test) style id3 fill:#f5f5f5,stroke:#6638B8,stroke-width:3px style id1 fill:#f5f5f5,stroke:#333,stroke-width:1px style id5 fill:#f5f5f5,stroke:#333,stroke-width:1px style id4 fill:#f5f5f5,stroke:#333,stroke-width:1px style id2 fill:#f5f5f5,stroke:#333,stroke-width:1px style id6 fill:#f5f5f5,stroke:#333,stroke-width:1px Use Streamlit to visualise your Snowplow data to make it easier to identify patterns and trends in your data.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/tracking/",
	"title": "Tracking",
	"tags": [],
	"description": "",
	"content": "Tracking flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Next steps) style id4 fill:#f5f5f5,stroke:#6638B8,stroke-width:3px style id1 fill:#f5f5f5,stroke:#333,stroke-width:1px style id3 fill:#f5f5f5,stroke:#333,stroke-width:1px style id5 fill:#f5f5f5,stroke:#333,stroke-width:1px style id2 fill:#f5f5f5,stroke:#333,stroke-width:1px Mobile hybrid apps implement some app logic in the platform native code (e.g., in Swift or Java) while some UI is implemented using embedded Web views. Since the two parts are developed using separate code bases, Snowplow events need to be tracked separately.\nThis part guides you to instrument Snowplow tracking in both the Web view and native mobile code and track events with consistent session and properties on both sides. It is structured in three parts:\nInstallation of the trackers in your apps. Instrumenting your native iOS or Android app with the mobile trackers and setting up the Web view communication. Tracking events from your Web view with the WebView tracker. "
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/next_steps/",
	"title": "Next steps",
	"tags": [],
	"description": "",
	"content": " Next steps flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Next steps) style id5 fill:#f5f5f5,stroke:#6638B8,stroke-width:3px style id1 fill:#f5f5f5,stroke:#333,stroke-width:1px style id3 fill:#f5f5f5,stroke:#333,stroke-width:1px style id4 fill:#f5f5f5,stroke:#333,stroke-width:1px style id2 fill:#f5f5f5,stroke:#333,stroke-width:1px Now that you have set-up tracking and enrichment on your pipeline and generated some test events it is time to make use of what you have learned so far by modelling and visualising your own pipeline data.\n"
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/",
	"title": "Snowplow Hybrid Apps Accelerator",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.snowplow.io/snowplow-hybrid-apps-accelerator/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]